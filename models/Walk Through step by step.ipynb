{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV file and examine its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/Data_Temperatures.csv'\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['Measurement Timestamp'] = pd.to_datetime(weather_data['Measurement Timestamp'])\n",
    "weather_data.set_index('Measurement Timestamp', inplace=True)\n",
    "\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = weather_data.pivot(columns='Station Name', values='Air Temperature')\n",
    "\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the 3 time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for station in df_pivot.columns:\n",
    "    plt.plot(df_pivot.index, df_pivot[station], label=station)\n",
    "\n",
    "plt.xlabel('Measurement Timestamp')\n",
    "plt.ylabel('Air Temperature (°C)')\n",
    "plt.title('Air Temperature Over Time for Each Weather Station')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random week and day within the dataset for specific plots\n",
    "# Define a specific day and a specific week for closer inspection\n",
    "one_day = df_pivot.loc['2016-01-01']\n",
    "one_week = df_pivot.loc['2016-01-01':'2016-01-07']\n",
    "\n",
    "# Plot for one day\n",
    "plt.figure(figsize=(10, 5))\n",
    "for station in one_day.columns:\n",
    "    plt.plot(one_day.index, one_day[station], label=station)\n",
    "plt.xlabel('Measurement Timestamp')\n",
    "plt.ylabel('Air Temperature (°C)')\n",
    "plt.title('Air Temperature Over Time for Each Weather Station (One Day)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot for one week\n",
    "plt.figure(figsize=(15, 8))\n",
    "for station in one_week.columns:\n",
    "    plt.plot(one_week.index, one_week[station], label=station)\n",
    "plt.xlabel('Measurement Timestamp')\n",
    "plt.ylabel('Air Temperature (°C)')\n",
    "plt.title('Air Temperature Over Time for Each Weather Station (One Week)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can observe some missing values, there are missing dates in may-june and maybe to some other places, let's give it a closer look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df_pivot.isna().sum()\n",
    "\n",
    "all_dates = pd.date_range(start=df_pivot.index.min(), end=df_pivot.index.max(), freq='H')\n",
    "missing_dates = all_dates.difference(df_pivot.index)\n",
    "\n",
    "missing_values, missing_dates.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates_by_station = {}\n",
    "\n",
    "for station in df_pivot.columns:\n",
    "    station_data = df_pivot[station]\n",
    "    station_data_reindexed = station_data.reindex(all_dates)\n",
    "    missing_dates = all_dates[station_data_reindexed.isna()]\n",
    "    missing_dates_by_station[station] = missing_dates\n",
    "\n",
    "for station, missing_dates in missing_dates_by_station.items():\n",
    "    missing_dates_df = pd.DataFrame(index=all_dates)\n",
    "    missing_dates_df['Missing'] = 0  # Default to 0 (not missing)\n",
    "    missing_dates_df.loc[missing_dates, 'Missing'] = 1  # Set to 1 for missing dates\n",
    "    \n",
    "    # Plot the missing dates over time\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(missing_dates_df.index, missing_dates_df['Missing'], marker='|', linestyle='None', color='red')\n",
    "    plt.title(f'Missing Timestamps for {station}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Missing (1 = Missing, 0 = Present)')\n",
    "    plt.yticks([0, 1], ['Present', 'Missing'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let s use interpolation to deal with the missing data. BUT BE CAREFUL, we want to keep the big gaps as NaN, interpolation would lower the accuracy of our model otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_gap_size = 24\n",
    "\n",
    "df_pivot = df_pivot.reindex(all_dates)\n",
    "\n",
    "large_gap_indices = {}\n",
    "\n",
    "for station in df_pivot.columns:\n",
    "    station_data = df_pivot[station]\n",
    "    \n",
    "    missing_mask = station_data.isna()\n",
    "    \n",
    "    gap_sizes = missing_mask.astype(int).groupby((~missing_mask).cumsum()).cumsum()\n",
    "    \n",
    "    large_gap_indices[station] = station_data[gap_sizes > max_gap_size].index\n",
    "\n",
    "    df_pivot[station] = station_data.interpolate() #df_pivot = df_pivot.interpolate() #fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    df_pivot.loc[large_gap_indices[station], station] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back up and run the graph -> reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok but I know these stations timeseries are highly correlated, I can do a test quickly here if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "correlations = {}\n",
    "for station1, station2 in combinations(df_pivot.columns, 2):\n",
    "    series1 = df_pivot[station1]\n",
    "    series2 = df_pivot[station2]\n",
    "    \n",
    "    correlation = series1.corr(series2)\n",
    "    correlations[(station1, station2)] = correlation\n",
    "    print(f\"Correlation between {station1} and {station2}: {correlation:.4f}\")\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for the rest of the missing data we use the avg of the other stations values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in df_pivot.columns:\n",
    "    missing_indices = df_pivot[station].isna()\n",
    "    \n",
    "    for idx in df_pivot[station][missing_indices].index:\n",
    "        available_data = [\n",
    "            df_pivot[other_station].loc[idx] for other_station in df_pivot.columns if other_station != station and not pd.isna(df_pivot[other_station].loc[idx])\n",
    "        ]\n",
    "        \n",
    "        if available_data:\n",
    "            df_pivot.loc[idx, station] = np.mean(available_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_names = df_pivot.columns\n",
    "lagged_data = pd.DataFrame()\n",
    "\n",
    "for station in station_names:\n",
    "    for lag in range(1, 8):  # 7 lags\n",
    "        lagged_data[f'{station}_lag{lag}'] = df_pivot[station].shift(lag)\n",
    "    lagged_data[f'{station}_lag{25}'] = df_pivot[station].shift(23)\n",
    "\n",
    "for station in station_names:\n",
    "    lagged_data[f'target_{station}'] = df_pivot[station]\n",
    "\n",
    "lagged_data = lagged_data.dropna()\n",
    "\n",
    "lagged_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's play with Random Forest nom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_data = lagged_data[lagged_data.index < '2016-12-01']\n",
    "val_data = lagged_data[(lagged_data.index >= '2016-12-01') & (lagged_data.index <= '2016-12-31')]\n",
    "\n",
    "mae_scores = {}\n",
    "forecast_results = {}\n",
    "\n",
    "for station in station_names:\n",
    "    X_train = train_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_train = train_data[f'target_{station}']\n",
    "    X_val = val_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_val = val_data[f'target_{station}']\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100)  # Hyperparameters can be tuned further\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae_scores[station] = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    forecast_results[station] = {\n",
    "        'train': y_train,\n",
    "        'validation': y_val,\n",
    "        'forecast': y_pred\n",
    "    }\n",
    "\n",
    "for station in station_names:\n",
    "    results = forecast_results[station]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results['train'].values, label=\"Train\")\n",
    "    plt.plot(range(len(results['train']), len(results['train']) + len(results['validation'])), \n",
    "             results['validation'].values, label=\"Validation\")\n",
    "    plt.plot(range(len(results['train']), len(results['train']) + len(results['forecast'])), \n",
    "             results['forecast'], label=\"Forecast\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast for {station} - MSE: {mae_scores[station]:.2f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Temperature (°C)\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"MAE scores for each station:\")\n",
    "mae_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = {}\n",
    "\n",
    "for station in station_names:\n",
    "\n",
    "    importances[station] = model.feature_importances_\n",
    "\n",
    "    feature_importance_series = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    sorted_importance = feature_importance_series.sort_values(ascending=False)\n",
    "    \n",
    "    sorted_importance.plot(kind='bar')\n",
    "    plt.title(f\"Feature Importance for {station}\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Importance Score\")\n",
    "    plt.show()\n",
    "\n",
    "for station, importance_values in importances.items():\n",
    "    print(f\"Top features for {station}:\")\n",
    "    sorted_features = sorted(zip(X_train.columns, importance_values), key=lambda x: x[1], reverse=True)\n",
    "    for feature, score in sorted_features[:5]:  # Show top 5 features\n",
    "        print(f\"{feature}: {score:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search with Out-of-Bag or val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "file_path = './data/Data_Temperatures.csv'\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n",
    "weather_data['Measurement Timestamp'] = pd.to_datetime(weather_data['Measurement Timestamp'])\n",
    "weather_data.set_index('Measurement Timestamp', inplace=True)\n",
    "\n",
    "df_pivot = weather_data.pivot(columns='Station Name', values='Air Temperature')\n",
    "df_pivot = df_pivot.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "station_names = df_pivot.columns\n",
    "lagged_data = pd.DataFrame()\n",
    "\n",
    "for station in station_names:\n",
    "    for lag in range(1, 8):  # 7 hours lag\n",
    "        lagged_data[f'{station}_lag{lag}'] = df_pivot[station].shift(lag)\n",
    "\n",
    "for station in station_names:\n",
    "    lagged_data[f'target_{station}'] = df_pivot[station]\n",
    "\n",
    "lagged_data = lagged_data.dropna()\n",
    "\n",
    "train_data = lagged_data[lagged_data.index < '2016-12-01']\n",
    "val_data = lagged_data[(lagged_data.index >= '2016-12-01') & (lagged_data.index <= '2016-12-31')]\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 201, 50), \n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_samples': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "best_params = {}\n",
    "mae_scores = {}\n",
    "forecast_results = {}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for station in station_names:\n",
    "    X_train = train_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_train = train_data[f'target_{station}']\n",
    "    X_val = val_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_val = val_data[f'target_{station}']\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        RandomForestRegressor(random_state=123, oob_score=True),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        random_state=123\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_station_params = random_search.best_params_\n",
    "    best_model = RandomForestRegressor(**best_station_params, random_state=123, oob_score=True)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = best_model.predict(X_val)\n",
    "    mae_scores[station] = mean_absolute_error(y_val, y_pred)\n",
    "    best_params[station] = best_station_params\n",
    "    \n",
    "    forecast_results[station] = {\n",
    "        'train': y_train,\n",
    "        'validation': y_val,\n",
    "        'forecast': y_pred\n",
    "    }\n",
    "\n",
    "for station in station_names:\n",
    "    results = forecast_results[station]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results['train'].values, label=\"Train\")\n",
    "    plt.plot(range(len(results['train']), len(results['train']) + len(results['validation'])), \n",
    "             results['validation'].values, label=\"Validation\")\n",
    "    plt.plot(range(len(results['train']), len(results['train']) + len(results['forecast'])), \n",
    "             results['forecast'], label=\"Forecast\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast for {station} - MAE: {mae_scores[station]:.2f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"MAE Scores per Station:\", mae_scores)\n",
    "print(\"Best Parameters per Station:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncerntainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = './data/Data_Temperatures.csv'\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n",
    "weather_data['Measurement Timestamp'] = pd.to_datetime(weather_data['Measurement Timestamp'])\n",
    "weather_data.set_index('Measurement Timestamp', inplace=True)\n",
    "\n",
    "df_pivot = weather_data.pivot(columns='Station Name', values='Air Temperature')\n",
    "df_pivot = df_pivot.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "station_names = df_pivot.columns\n",
    "lagged_data = pd.DataFrame()\n",
    "\n",
    "for station in station_names:\n",
    "    for lag in range(1, 8):  # 7 hours lag\n",
    "        lagged_data[f'{station}_lag{lag}'] = df_pivot[station].shift(lag)\n",
    "\n",
    "for station in station_names:\n",
    "    lagged_data[f'target_{station}'] = df_pivot[station]\n",
    "\n",
    "lagged_data = lagged_data.dropna()\n",
    "\n",
    "train_data = lagged_data[lagged_data.index < '2016-12-01']\n",
    "val_data = lagged_data[(lagged_data.index >= '2016-12-01') & (lagged_data.index <= '2016-12-31')]\n",
    "\n",
    "mae_scores = {}\n",
    "forecast_results = {}\n",
    "\n",
    "for station in station_names:\n",
    "    X_train = train_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_train = train_data[f'target_{station}']\n",
    "    X_val = val_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_val = val_data[f'target_{station}']\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=123) #max_depth=10, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', , oob_score=True\n",
    "                                  \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    all_tree_predictions = np.array([tree.predict(X_val.values) for tree in model.estimators_])\n",
    "\n",
    "    mean_predictions = np.mean(all_tree_predictions, axis=0)\n",
    "    std_predictions = np.std(all_tree_predictions, axis=0)\n",
    "\n",
    "    mae_scores[station] = mean_absolute_error(y_val, mean_predictions)\n",
    "    \n",
    "    forecast_results[station] = {\n",
    "        'validation': y_val,\n",
    "        'forecast': mean_predictions,\n",
    "        'std': std_predictions\n",
    "    }\n",
    "\n",
    "for station in station_names:\n",
    "    results = forecast_results[station]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results['validation'].index, results['validation'].values, label=\"Actual (Validation)\", color=\"green\")\n",
    "    plt.plot(results['validation'].index, results['forecast'], label=\"Mean Prediction\", color=\"orange\")\n",
    "    plt.fill_between(results['validation'].index, \n",
    "                     results['forecast'] - results['std'], \n",
    "                     results['forecast'] + results['std'], \n",
    "                     color=\"orange\", alpha=0.2, label=\"Prediction ± 1 std\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"December Forecast for {station} - MAE: {mae_scores[station]:.2f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "print(\"MAE Scores per Station:\", mae_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To improve:\n",
    "- Add more features, be smarter with the features, it's the most important part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
