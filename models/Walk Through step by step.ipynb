{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV file and examine its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/natha/OneDrive/Bureau/Interview trainings/Coding/Aquatic/Weather_Forecast_Ideas/data/chicago_beach_weather.csv'\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['Measurement Timestamp'] = pd.to_datetime(weather_data['Measurement Timestamp'])\n",
    "weather_data.set_index('Measurement Timestamp', inplace=True)\n",
    "\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = weather_data.pivot(columns='Station Name', values='Air Temperature')\n",
    "\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the 3 time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for station in df_pivot.columns:\n",
    "    plt.plot(df_pivot.index, df_pivot[station], label=station)\n",
    "\n",
    "plt.xlabel('Measurement Timestamp')\n",
    "plt.ylabel('Air Temperature (°C)')\n",
    "plt.title('Air Temperature Over Time for Each Weather Station')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random week and day within the dataset for specific plots\n",
    "# Define a specific day and a specific week for closer inspection\n",
    "one_day = df_pivot.loc['2016-01-01']\n",
    "one_week = df_pivot.loc['2016-01-01':'2016-01-07']\n",
    "\n",
    "# Plot for one day\n",
    "plt.figure(figsize=(10, 5))\n",
    "for station in one_day.columns:\n",
    "    plt.plot(one_day.index, one_day[station], label=station)\n",
    "plt.xlabel('Measurement Timestamp')\n",
    "plt.ylabel('Air Temperature (°C)')\n",
    "plt.title('Air Temperature Over Time for Each Weather Station (One Day)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot for one week\n",
    "plt.figure(figsize=(15, 8))\n",
    "for station in one_week.columns:\n",
    "    plt.plot(one_week.index, one_week[station], label=station)\n",
    "plt.xlabel('Measurement Timestamp')\n",
    "plt.ylabel('Air Temperature (°C)')\n",
    "plt.title('Air Temperature Over Time for Each Weather Station (One Week)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can observe some missing values, there are missing dates in may-june and maybe to some other places, let's give it a closer look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df_pivot.isna().sum()\n",
    "\n",
    "all_dates = pd.date_range(start=df_pivot.index.min(), end=df_pivot.index.max(), freq='H')\n",
    "missing_dates = all_dates.difference(df_pivot.index)\n",
    "\n",
    "missing_values, missing_dates.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine missing dates by station\n",
    "missing_dates_by_station = {}\n",
    "\n",
    "# Identify missing dates for each station\n",
    "for station in df_pivot.columns:\n",
    "    station_data = df_pivot[station]\n",
    "    station_data_reindexed = station_data.reindex(all_dates)\n",
    "    missing_dates = all_dates[station_data_reindexed.isna()]\n",
    "    missing_dates_by_station[station] = missing_dates\n",
    "\n",
    "# Plot the missing dates for each station\n",
    "for station, missing_dates in missing_dates_by_station.items():\n",
    "    # Create a DataFrame to represent missing dates for visualization\n",
    "    missing_dates_df = pd.DataFrame(index=all_dates)\n",
    "    missing_dates_df['Missing'] = 0  # Default to 0 (not missing)\n",
    "    missing_dates_df.loc[missing_dates, 'Missing'] = 1  # Set to 1 for missing dates\n",
    "    \n",
    "    # Plot the missing dates over time\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(missing_dates_df.index, missing_dates_df['Missing'], marker='|', linestyle='None', color='red')\n",
    "    plt.title(f'Missing Timestamps for {station}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Missing (1 = Missing, 0 = Present)')\n",
    "    plt.yticks([0, 1], ['Present', 'Missing'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok but I know these stations timeseries are highly correlated, I can do a test quickly here if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Calculate correlations between each pair of stations in df_pivot\n",
    "correlations = {}\n",
    "for station1, station2 in combinations(df_pivot.columns, 2):\n",
    "    series1 = df_pivot[station1]\n",
    "    series2 = df_pivot[station2]\n",
    "    \n",
    "    correlation = series1.corr(series2)\n",
    "    correlations[(station1, station2)] = correlation\n",
    "    print(f\"Correlation between {station1} and {station2}: {correlation:.4f}\")\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in df_pivot.columns:\n",
    "    # Identify missing indices in the current station's data\n",
    "    missing_indices = df_pivot[station].isna()\n",
    "    \n",
    "    # For each missing index, calculate the average from available correlated stations\n",
    "    for idx in df_pivot[station][missing_indices].index:\n",
    "        # Collect data from other stations at the same timestamp if available\n",
    "        available_data = [\n",
    "            df_pivot[other_station].loc[idx] for other_station in df_pivot.columns if other_station != station and not pd.isna(df_pivot[other_station].loc[idx])\n",
    "        ]\n",
    "        \n",
    "        # If data from other stations is available, fill with the average\n",
    "        if available_data:\n",
    "            df_pivot.loc[idx, station] = np.mean(available_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back up and run the graph -> reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then interpolate the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAREFUL THIS ONE IS FOR ALL\n",
    "'''\n",
    "# Re-index to ensure all dates are present (including missing timestamps)\n",
    "df_pivot = df_pivot.reindex(all_dates)\n",
    "\n",
    "# Interpolate missing values linearly\n",
    "df_pivot.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Display head to verify\n",
    "df_pivot.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum gap size for interpolation (e.g., 5 consecutive hours)\n",
    "max_gap_size = 24\n",
    "\n",
    "# Reindex to ensure all timestamps are present\n",
    "df_pivot = df_pivot.reindex(all_dates)\n",
    "\n",
    "# Loop through each station to identify and store large gaps\n",
    "large_gap_indices = {}\n",
    "\n",
    "for station in df_pivot.columns:\n",
    "    station_data = df_pivot[station]\n",
    "    \n",
    "    # Create a mask for missing values\n",
    "    missing_mask = station_data.isna()\n",
    "    \n",
    "    # Calculate the size of consecutive missing gaps\n",
    "    gap_sizes = missing_mask.astype(int).groupby((~missing_mask).cumsum()).cumsum()\n",
    "    \n",
    "    # Store indices where the gap is larger than max_gap_size\n",
    "    large_gap_indices[station] = station_data[gap_sizes > max_gap_size].index\n",
    "\n",
    "    # Interpolate missing values for this station\n",
    "    df_pivot[station] = station_data.interpolate() #df_pivot = df_pivot.interpolate() #fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Reapply NaN to the large gaps in this station\n",
    "    df_pivot.loc[large_gap_indices[station], station] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the graph -> everything disappeared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged data for time series forecasting on all stations\n",
    "station_names = df_pivot.columns\n",
    "lagged_data = pd.DataFrame()\n",
    "\n",
    "# Generate lagged features for each station\n",
    "for station in station_names:\n",
    "    for lag in range(1, 8):  # 7 lag days\n",
    "        lagged_data[f'{station}_lag{lag}'] = df_pivot[station].shift(lag) # df_pivot[station].shift(24) can be interesting to add\n",
    "    lagged_data[f'{station}_lag{25}'] = df_pivot[station].shift(25)\n",
    "# Set target variables for each station\n",
    "for station in station_names:\n",
    "    lagged_data[f'target_{station}'] = df_pivot[station]\n",
    "\n",
    "# Drop rows with any NaN values (due to lagging)\n",
    "lagged_data = lagged_data.dropna()\n",
    "\n",
    "# Display the head of the resulting lagged dataset\n",
    "lagged_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's play with Random Forest nom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split data into training (before December) and validation (December only)\n",
    "train_data = lagged_data[lagged_data.index < '2016-12-01']\n",
    "val_data = lagged_data[(lagged_data.index >= '2016-12-01') & (lagged_data.index <= '2016-12-31')]\n",
    "\n",
    "mae_scores = {}\n",
    "forecast_results = {}\n",
    "\n",
    "for station in station_names:\n",
    "    # Prepare training and validation sets for the current station\n",
    "    X_train = train_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_train = train_data[f'target_{station}']\n",
    "    X_val = val_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_val = val_data[f'target_{station}']\n",
    "\n",
    "    # Train a Random Forest Regressor for each station\n",
    "    model = RandomForestRegressor(n_estimators=100)  # Hyperparameters can be tuned further\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae_scores[station] = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    # Store results for plotting\n",
    "    forecast_results[station] = {\n",
    "        'train': y_train,\n",
    "        'validation': y_val,\n",
    "        'forecast': y_pred\n",
    "    }\n",
    "\n",
    "# Plot results for each station\n",
    "for station in station_names:\n",
    "    results = forecast_results[station]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results['train'].values, label=\"Train\")\n",
    "    plt.plot(range(len(results['train']), len(results['train']) + len(results['validation'])), \n",
    "             results['validation'].values, label=\"Validation\")\n",
    "    plt.plot(range(len(results['train']), len(results['train']) + len(results['forecast'])), \n",
    "             results['forecast'], label=\"Forecast\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast for {station} - MSE: {mae_scores[station]:.2f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Temperature (°C)\")\n",
    "    plt.show()\n",
    "\n",
    "# Print Mean Absolute Error scores for each station\n",
    "print(\"MAE scores for each station:\")\n",
    "mae_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "\n",
    "from multi_output_module import Multi_output_module\n",
    "\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_ood = torch.tensor(X_ood, dtype=torch.float32)\n",
    "y_ood = torch.tensor(y_ood, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "ood_dataset = TensorDataset(X_ood, y_ood)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "ood_loader = DataLoader(ood_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Function to train the model with early stopping\n",
    "def train_model_with_early_stopping(model, train_loader, val_loader, epochs=100, patience=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = evaluate_model(model, val_loader)\n",
    "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Check for early stopping based on validation loss\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming train_loader and val_loader are already defined\n",
    "for i, model in enumerate(ensemble_models):\n",
    "    print(f'Training model {i+1}/{ensemble_size}')\n",
    "    train_model_with_early_stopping(model, train_loader, val_loader, epochs=num_epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = {}\n",
    "\n",
    "for station in station_names:\n",
    "\n",
    "    importances[station] = model.feature_importances_\n",
    "\n",
    "    # Plot feature importance\n",
    "    feature_importance_series = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    sorted_importance = feature_importance_series.sort_values(ascending=False)\n",
    "    \n",
    "    sorted_importance.plot(kind='bar')\n",
    "    plt.title(f\"Feature Importance for {station}\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Importance Score\")\n",
    "    plt.show()\n",
    "\n",
    "# Print a summary of the top features for each station\n",
    "for station, importance_values in importances.items():\n",
    "    print(f\"Top features for {station}:\")\n",
    "    sorted_features = sorted(zip(X_train.columns, importance_values), key=lambda x: x[1], reverse=True)\n",
    "    for feature, score in sorted_features[:5]:  # Show top 5 features\n",
    "        print(f\"{feature}: {score:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search with Out-of-Bag or val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'C:/Users/natha/OneDrive/Bureau/Interview trainings/Coding/Aquatic/Weather_Forecast_Ideas/data/chicago_beach_weather.csv'\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "weather_data['Measurement Timestamp'] = pd.to_datetime(weather_data['Measurement Timestamp'])\n",
    "weather_data.set_index('Measurement Timestamp', inplace=True)\n",
    "\n",
    "# Pivot the data by station name and fill any missing values\n",
    "df_pivot = weather_data.pivot(columns='Station Name', values='Air Temperature')\n",
    "df_pivot = df_pivot.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Generate lagged features\n",
    "station_names = df_pivot.columns\n",
    "lagged_data = pd.DataFrame()\n",
    "\n",
    "# Create lagged features for each station\n",
    "for station in station_names:\n",
    "    for lag in range(1, 8):  # 7 hours lag\n",
    "        lagged_data[f'{station}_lag{lag}'] = df_pivot[station].shift(lag)\n",
    "\n",
    "# Add each station's current value as a target\n",
    "for station in station_names:\n",
    "    lagged_data[f'target_{station}'] = df_pivot[station]\n",
    "\n",
    "# Drop rows with NaN values after creating lags\n",
    "lagged_data = lagged_data.dropna()\n",
    "\n",
    "# Split data into training (before December) and validation (December only)\n",
    "train_data = lagged_data[lagged_data.index < '2016-12-01']\n",
    "val_data = lagged_data[(lagged_data.index >= '2016-12-01') & (lagged_data.index <= '2016-12-31')]\n",
    "\n",
    "# Define the parameter grid for hyperparameter optimization\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 201, 50),  # Try 50, 100, 150, 200\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_samples': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Dictionary to store results for each station\n",
    "best_params = {}\n",
    "mae_scores = {}\n",
    "forecast_results = {}\n",
    "\n",
    "# Use a time series split for cross-validation to preserve the temporal order\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Train and predict for each station\n",
    "for station in station_names:\n",
    "    X_train = train_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_train = train_data[f'target_{station}']\n",
    "    X_val = val_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_val = val_data[f'target_{station}']\n",
    "    \n",
    "    # Perform random search with cross-validation\n",
    "    random_search = RandomizedSearchCV(\n",
    "        RandomForestRegressor(random_state=123, oob_score=True),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  # Number of parameter settings to sample\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        random_state=123\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Train the final model using the best parameters\n",
    "    best_station_params = random_search.best_params_\n",
    "    best_model = RandomForestRegressor(**best_station_params, random_state=123, oob_score=True)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and compute MAE for the validation set\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    mae_scores[station] = mean_absolute_error(y_val, y_pred)\n",
    "    best_params[station] = best_station_params\n",
    "    \n",
    "    # Store the forecast, validation, and training sets for plotting\n",
    "    forecast_results[station] = {\n",
    "        'train': y_train,\n",
    "        'validation': y_val,\n",
    "        'forecast': y_pred\n",
    "    }\n",
    "\n",
    "# Plot the forecast, validation, and training sets for each station\n",
    "for station in station_names:\n",
    "    results = forecast_results[station]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results['train'].values, label=\"Train\")\n",
    "    plt.plot(range(len(results['train']), len(results['train']) + len(results['validation'])), \n",
    "             results['validation'].values, label=\"Validation\")\n",
    "    plt.plot(range(len(results['train']), len(results['train']) + len(results['forecast'])), \n",
    "             results['forecast'], label=\"Forecast\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Forecast for {station} - MAE: {mae_scores[station]:.2f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.show()\n",
    "\n",
    "# Display MAE scores and best parameters for each station\n",
    "print(\"MAE Scores per Station:\", mae_scores)\n",
    "print(\"Best Parameters per Station:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncerntainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'C:/Users/natha/OneDrive/Bureau/Interview trainings/Coding/Aquatic/Weather_Forecast_Ideas/data/chicago_beach_weather.csv'\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "weather_data['Measurement Timestamp'] = pd.to_datetime(weather_data['Measurement Timestamp'])\n",
    "weather_data.set_index('Measurement Timestamp', inplace=True)\n",
    "\n",
    "# Pivot the data by station name and fill any missing values\n",
    "df_pivot = weather_data.pivot(columns='Station Name', values='Air Temperature')\n",
    "df_pivot = df_pivot.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Generate lagged features\n",
    "station_names = df_pivot.columns\n",
    "lagged_data = pd.DataFrame()\n",
    "\n",
    "# Create lagged features for each station\n",
    "for station in station_names:\n",
    "    for lag in range(1, 8):  # 7 hours lag\n",
    "        lagged_data[f'{station}_lag{lag}'] = df_pivot[station].shift(lag)\n",
    "\n",
    "# Add each station's current value as a target\n",
    "for station in station_names:\n",
    "    lagged_data[f'target_{station}'] = df_pivot[station]\n",
    "\n",
    "# Drop rows with NaN values after creating lags\n",
    "lagged_data = lagged_data.dropna()\n",
    "\n",
    "# Split data into training (before December) and validation (December only)\n",
    "train_data = lagged_data[lagged_data.index < '2016-12-01']\n",
    "val_data = lagged_data[(lagged_data.index >= '2016-12-01') & (lagged_data.index <= '2016-12-31')]\n",
    "\n",
    "# Dictionary to store results for each station\n",
    "mae_scores = {}\n",
    "forecast_results = {}\n",
    "\n",
    "# Train and predict for each station\n",
    "for station in station_names:\n",
    "    X_train = train_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_train = train_data[f'target_{station}']\n",
    "    X_val = val_data.drop(columns=[f'target_{s}' for s in station_names])\n",
    "    y_val = val_data[f'target_{station}']\n",
    "    \n",
    "    # Train a single RandomForestRegressor model with predefined parameters\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=123) #max_depth=10, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', , oob_score=True\n",
    "                                  \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions from each tree in the forest for the validation set\n",
    "    all_tree_predictions = np.array([tree.predict(X_val.values) for tree in model.estimators_])\n",
    "\n",
    "    # Calculate mean and standard deviation of predictions across all trees\n",
    "    mean_predictions = np.mean(all_tree_predictions, axis=0)\n",
    "    std_predictions = np.std(all_tree_predictions, axis=0)\n",
    "\n",
    "    # Compute MAE for the mean predictions\n",
    "    mae_scores[station] = mean_absolute_error(y_val, mean_predictions)\n",
    "    \n",
    "    # Store the forecast, validation, and training sets for plotting\n",
    "    forecast_results[station] = {\n",
    "        'validation': y_val,\n",
    "        'forecast': mean_predictions,\n",
    "        'std': std_predictions\n",
    "    }\n",
    "\n",
    "# Plot only the December forecast with uncertainty band for each station\n",
    "for station in station_names:\n",
    "    results = forecast_results[station]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results['validation'].index, results['validation'].values, label=\"Actual (Validation)\", color=\"green\")\n",
    "    plt.plot(results['validation'].index, results['forecast'], label=\"Mean Prediction\", color=\"orange\")\n",
    "    plt.fill_between(results['validation'].index, \n",
    "                     results['forecast'] - results['std'], \n",
    "                     results['forecast'] + results['std'], \n",
    "                     color=\"orange\", alpha=0.2, label=\"Prediction ± 1 std\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"December Forecast for {station} - MAE: {mae_scores[station]:.2f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Display MAE scores for each station\n",
    "print(\"MAE Scores per Station:\", mae_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To improve:\n",
    "- More robust cross validation\n",
    "- Add more features\n",
    "- try xgboost / ensemble of xgboost, TFT\n",
    "- Question about testing methodology "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
